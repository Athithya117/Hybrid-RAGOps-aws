
# RAG8s is a production ready E2E RAG system built using the SOTA tools, models and strategies as of mid 2025. 






## STEP 2/3 - indexing_pipeline

#### **NVIDIA (June 2025)** : Page-level chunking is the baseline best https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/

> â€œPage-level chunking is the overall winner: Our experiments clearly show that page-level chunking achieved the highest average accuracy (0.648) across all datasets and the lowest standard deviation (0.107), showing more consistent performance across different content types. Page-level chunking demonstrated superior overall performance when compared to both token-based chunking and section-level chunking.â€ 

#### RAG8s implements page-wise chunking and similar chunking for scalability without losing accuracy
<details>
  <summary> View chunk stratergies and chunk schema (Click the triangle)</summary>

```sh

{
  "chunk_id": "a3f5be12c9d47e09_5",               // Unique chunk ID: <document_hash>_<chunk_index> (1-based)
  "document_id": "a3f5be12c9d47e09",              // Unique document ID (128-bit hash of file path + size)
  "chunk_type": "page",                           // Type of content: "paragraph", "heading", "section", "table", "page", etc.

  "text": "Cleaned markdown-formatted content of the chunk.",  // Final parsed content (Markdown)
  "embedding": null,                              // Optional: vector embedding (array of floats); null if not yet computed

  "source": {
    "file_type": "application/pdf",                  // MIME type preferred (e.g., "application/pdf", "text/html", "audio/mpeg")
    "source_path": "s3://bucket/data/raw/file.pdf",  // Full s3 path to original source
    "page_number": 3,                                // For paged formats like PDF/ePub; null otherwise
    "time": [null, null],                            // [start_time, end_time] in seconds for audio/video; nulls otherwise
    "line_range": null,                              // For plain/tabular text: [start_line, end_line]; null otherwise
    "bbox": null                                     // For visual formats: [x0, y0, x1, y1] in pixel coordinates; null otherwise
  },

  "graph": {
    "graph_node_id": "a3f5be12c9d47e09_5",       // Same as chunk_id (recommended)
    "parent_id": "a3f5be12c9d47e09_page3",       // Parent node ID (e.g., page, section, table)
    "previous_id": "a3f5be12c9d47e09_4",         // Optional: previous chunk
    "next_id": "a3f5be12c9d47e09_6"              // Optional: next chunk
  },

  "metadata": {
    "timestamp": "2025-08-03T12:00:00Z",         // UTC ISO timestamp of chunk creation/parsing
    "tags": ["invoice", "header"],               // High-level content tags (semantic or manual)
    "layout_tags": ["paragraph"]                 // Structural tags (e.g., "heading", "table", etc.)
  },

  "entities": ["Q123", "Q456"],                  // Optional: Linked entity IDs (Wikidata, etc.) or null if not yet computed

  "triplets": [                                  // Extracted subject-predicate-object relations
    {
      "subject": "Invoice",
      "predicate": "hasDate",
      "object": "2025-08-01"
    },
    {
      "subject": "Invoice",
      "predicate": "issuedBy",
      "object": "ACME Corp"
    },
    {
      "subject": "ACME Corp",
      "predicate": "locatedIn",
      "object": "New York"
    }
  ]
}


```


## Component-Level Parsing & Chunking Strategy

| Component            | Tool(s) Used                                                                                                  | Chunking Strategy                                                                                                         | Rationale for Scalability & Fidelity                                                                 |
|----------------------|---------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|
| **PDF Parsing + OCR** | `PyMuPDF`, `pdfplumber`, `pytesseract`, `RapidOCR` | - **1 page = 1 chunk** (primary granularity)<br>- Extract native text via `pdfplumber`, fallback to OCR if needed<br>- Use `OpenCV` for layout-aware OCR (cropping regions, columnar reading)<br>- Tables extracted as both **structured JSON** and **inline Markdown**<br>- Figures/diagrams extracted to a separate `figures` array | - Page-level chunking aligns with layout and enables **parallelism**<br>- Hybrid OCR improves coverage of low-quality scans<br>- Layout fidelity helps preserve **tables, headers, visual order**                             |
| **DOC/DOCX Conversion** | `LibreOffice` (headless mode), `subprocess`, `tempfile`, `boto3`, `pathlib`, `os`                             | - Convert `.doc` â†’ `.docx` â†’ `.pdf` using `LibreOffice CLI`<br>- Apply **same PDF+OCR pipeline per page** on output PDF<br>- Page alignment maintained between original and converted formats                        | - Avoids unreliable native `.docx` parsing<br>- Ensures **visual and semantic consistency** across systems<br>- Helps in tracing                                                   |
| **Text/HTML Parsing** | `BeautifulSoup`, `html2text`, custom chunkers                                                                 | - Segment by structural tags: `h1â€“h6`, `p`, `li`, `div`, `table`<br>- Normalize output into **Markdown chunks**<br>- Chunk IDs and parent-child relations inferred from tag hierarchy                         | - Converts semi-structured content into **RAG-ready Markdown**<br>- Retains **hierarchy and inline metadata** for graph linkage<br>- Works well with multi-format source ingestion                      |                                            |
| **HTML Parsing**        | extractous, BeautifulSoup                                                        | - Parse HTML DOM tree  <br> - Chunk by headings (`<h1>`â€“`<h6>`), paragraphs, and sections                                                                                                                            | - Lightweight, preserves semantic structure  <br> - Works on both web pages and embedded HTML                                     |
| **CSV Chunking**        | ray.data.read\_csv(), `.window()`                                                | - Stream rows  <br> - Chunk based on size heuristics (`max(5120, avg_row_len * ROWS_PER_CHUNK)`)                                                                                                                     | - Efficient streaming for large files  <br> - Memory-safe, scalable via Ray                                                       |
| **JSON/JSONL**          | ray.data.read\_json(), `.window()`                                               | - JSONL: each line = record  <br> - For nested JSON: flatten â†’ explode arrays â†’ chunk by size/depth                                                                                                                  | - Handles deeply nested or irregular structures  <br> - Flexible chunk size based on token count                                  |
| **Audio Transcription** | faster-whisper (CTranslate2), pydub, ffmpeg-python                               | - Audio sliced into 20â€“30s segments via silence detection (`pydub.silence`)  <br> - Each segment transcribed individually                                                                                            | - Faster-Whisper is GPU/CPU efficient  <br> - Segmentation makes long audio scalable and parallelizable                           |
| **Markdown**            | markdown-it-py, mistune, regex                                                   | - Chunk by heading levels, paragraphs, and code blocks  <br> - Fallback to fixed-token or sentence-based slicing                                                                                                     | - Preserves Markdown structure  <br> - Compatible with LLM indexing and embeddings                                                |
| **PPTX (PowerPoint)**   | python-pptx, Pillow (optional OCR)                                               | - 1 slide = 1 chunk  <br> - Extract text, speaker notes, images  <br> - OCR fallback on image slides                                                                                                                 | - Natural chunking by slide  <br> - Works well with educational or slide-heavy documents                                          |
| **EPUB/eBooks**         | ebooklib, BeautifulSoup, html5lib                                                | - Chunk by chapters/headings from EPUB metadata  <br> - Paragraph or heading-based segmentation within chapters                                                                                                      | - Structure-aware  <br> - Works with long-form content like books                                                                 |
| **Images (Scans)**      | OpenCV, PIL/Pillow, Tesseract or RapidOCR                                        | - 1 image = 1 chunk  <br> - OCR applied to entire image or regions (if detected)                                                                                                                                     | - Useful for form scans, handwritten notes, flyers  <br> - Preserves visual layout                                                |
| **ZIP Archives**        | zipfile, tarfile, custom dispatcher                                              | - Files extracted, routed to correct parsers based on extension (pdf, docx, txt, etc.)                                                                                                                               | - Allows batch ingestion  <br> - Enables unified multi-file upload experience                                                     |
| **Plaintext Files**     | open(), re, nltk, tiktoken (optional)                                            | - Chunk by paragraph, newline gaps (`\n\n`), or fixed line/token window                                                                                                                                              | - Extremely lightweight  <br> - Works well with logs, scraped data, or long articles                                              |

</details>


### Export the neccessary configs.

```sh

# Storage / paths
export S3_BUCKET=e2e-rag-system                      # set per-env/tenant (unique bucket)
export S3_RAW_PREFIX=data/raw/                        # raw ingest prefix (change to isolate datasets)
export S3_CHUNKED_PREFIX=data/chunked/                # chunked output prefix (change to separate processed data)
export CHUNK_FORMAT=json                              # 'json' (readable) or 'jsonl' (stream/space efficient)
export OVERWRITE_DOC_DOCX_TO_PDF=true                 # true to replace docx with PDF, false to keep originals

# OCR & image extraction
export DISABLE_OCR=false                              # true to skip OCR (faster) | false to extract text from images
export OCR_ENGINE=tesseract                           # 'tesseract' (fast/common) or 'rapidocr' (higher accuracy, slower)
export FORCE_OCR=false                                # true to always OCR (use if source text unreliable)
export OCR_RENDER_DPI=300                             # increase for tiny text; lower for speed
export MIN_IMG_SIZE_BYTES=3072                        # ignore images smaller than this (reduce noise)


```

```sh 

# Arango / vector index toggles
export ARANGO_VECTOR_INDEX_ENABLE=true                # range: true|false; false to disable vector ops (read-only or minimal infra)
export ARANGO_VECTOR_INDEX_TYPE="ivf"                 # range: 'hnsw'|'ivf'|'ivf+pq'; choose 'hnsw' (<100k docs), 'ivf' (>=100k), 'ivf+pq' for huge corpora
export ARANGO_VECTOR_INDEX_MAX_MEMORY_MB=2048         # range: 512-65536 MB; soft cap for index memory on node; increase with corpus size

# IVF-specific (only if using ivf)
export ARANGO_VECTOR_INDEX_IVF_NLIST=1000             # range: 256-16384; set ~sqrt(N_vectors); increase for very large corpora
export ARANGO_VECTOR_INDEX_IVF_NPROBE=10              # range: 4-128; raise for recall at cost of latency

# PQ (only if using ivf+pq/pq)
export ARANGO_VECTOR_INDEX_PQ_M=16                    # range: 8-32; PQ segments; must divide embedding dim; tune for memory vs accuracy

# HNSW-specific (only if using hnsw)
export ARANGO_VECTOR_INDEX_HNSW_M=32                  # range: 16-64; higher => more memory but higher recall
export ARANGO_VECTOR_INDEX_HNSW_EFCONSTRUCTION=200    # range: 100-800; raise for better index build quality
export ARANGO_VECTOR_INDEX_HNSW_EFSEARCH=50           # range: 40-300; raise for higher query recall (latency â†‘)

# FAISS sidecar / local index
export FAISS_INDEX_PATH="/mnt/faiss/index.ivf"        # range: filesystem path|"empty"; local index path (empty if not used)
export FAISS_INDEX_DIM=768                            # range: embedding dim; must match embedding model output
export FAISS_NLIST=256                                # range: 128-16384; local FAISS nlist; increase for large indices
export FAISS_NPROBE=10                                # range: 1-128; raise for recall at latency cost

# Retrieval fusion weights (tune by devset; relative importance)
export W_VEC=0.6                                      # range: 0.0-1.0; raise if domain embeddings are highly accurate
export W_BM25=0.3                                     # range: 0.0-1.0; raise if exact keyword matches are critical
export W_GRAPH=0.1                                    # range: 0.0-1.0; raise if graph/triplet hits are very high precision
export W_RERANK=0.5                                   # range: 0.0-1.0; meaningful only when reranker enabled

# Candidate fanout & GeAR
export N_VEC=15                                       # range: 5-100; top-K vector candidates (raise for recall on large corpora)
export N_BM25=15                                      # range: 5-100; top-K BM25 candidates
export N_GRAPH=5                                      # range: 1-10; graph neighbor limit (keep small to control DB load)
export MAX_GEARS_HOPS=1                               # range: 1-2; 1 default; enable 2 behind feature flag for deeper multi-hop
export GEAR_BEAM_WIDTH=3                              # range: 1-5; beam width for GeAR expansion; increase with caution

# Pre-fusion thresholds (filters to reduce noise)
export VEC_SCORE_THRESH=0.20                          # range: 0.05-0.40; min vector similarity to keep a candidate (raise for precision)
export BM25_SCORE_THRESH=1.50                         # range: 0.5-3.0; min BM25 to keep (raise to filter weak keyword hits)
export GRAPH_SCORE_THRESH=0.0                         # range: 0.0-0.5; min graph edge confidence (set >0 if confidences provided)

# Reranker & metadata boosting
export USE_RERANKER=true                              # range: true|false; enable only if you accept added latency/cost for higher precision
export RERANK_BATCH_SIZE=16                           # range: 4-64; increase to amortize GPU/CPU when latency allows
export META_BOOST_FIELD="timestamp"                   # range: metadata key name; metadata key to bias ranking (e.g., timestamp, source_score)
export META_BOOST_WEIGHT=0.20                         # range: 0.0-1.0; raise if metadata should strongly affect ranking

```

# Timeouts / concurrency / performance
export RETRIEVAL_TIMEOUT=5                            # seconds; increase if backing systems are slower
export RETRIEVAL_BATCH_SIZE=4                         # parallelism for retrieval calls; increase with CPU/network capacity
export MAX_CONCURRENT_QUERIES=32                      # throttle to protect DBs; scale with infra
# Arango general performance / logging
export ARANGO_STORAGE_CACHE_SIZE=2048                 # set ~20-30% host RAM for read-heavy nodes
export ARANGO_QUERY_MEMORY_LIMIT=1024                 # raise if AQL traversals need more memory
export ARANGO_LOG_LEVEL="info"                        # 'debug' only for troubleshooting
export ARANGO_URL="http://arangodb:8529"              # Arango endpoint (use in-cluster svc in prod)
export ARANGO_DB="rag8s"                              # DB name
export ARANGO_USER="root"                             # use non-root least-priv user in prod
export ARANGO_PASS=""                                 # SECRET: inject from Vault/K8s Secret


# Ray / orchestration (node-level)
export RAY_DASHBOARD_PORT=8265                        # Ray dashboard port (change to avoid conflicts)
export RAY_NUM_CPUS=4                                 # set per node; increase for heavier parallel indexing/inference
export RAY_NUM_GPUS=0                                 # set >0 on GPU nodes for model serving/training

# Observability / app logging
export APP_LOG_LEVEL="info"                           # 'debug' only temporarily for troubleshooting

```



```sh

User query
    â”‚
    â”œâ”€â†’ arangosearch â†’ candidates
    â”œâ”€â†’ Vector search â†’ semantically similar candidates
    â””â”€â†’ Graph (ArangoDB) + GeAR â†’ multi-hop linked candidates
            â†“
        Candidate merger & reranker (optional)
            â†“
        LLM generation / RAG output

Graph traversal â†’ â€œFollow the edges and return everything connected.â€

GeAR reasoning â†’ â€œUse the graph + triplets to infer which paths and nodes are actually relevant to the query, even if not directly connected.â€
FAISS handles â€œmeaning in text,â€ GeAR handles â€œmeaning in structure.â€ Both are needed for a hybrid RAG.


```


### The RAG8s platform codebase


```py

RAG8s/
â”œâ”€â”€ data/                                 # Local directory that syncs with s3://<bucket_name>/data
â”‚   â”œâ”€â”€ raw/                              # Raw data files
â”‚   â””â”€â”€ chunked/                          # Chunks in json/jsonl format
â”‚
â”œâ”€â”€ indexing_pipeline/
â”‚   â”œâ”€â”€ Dockerfile                        # Docker image for indexing workers
â”‚   â”œâ”€â”€ index/
â”‚   â”‚   â”œâ”€â”€ __main__.py                   # CLI entrypoint for indexing jobs  # observe: logs, metrics, traces
â”‚   â”‚   â”œâ”€â”€ arrangodb_indexer.py          # Indexer: writes chunks/entities into ArangoDB with FAISS integration  # observe: logs, metrics
â”‚   â”‚   â”œâ”€â”€ config.py                     # Indexing configuration (paths, batch sizes, env)
â”‚   â”‚   â””â”€â”€ utils.py                      # Utility helpers used by indexers (parsers, serializers)
â”‚   â”œâ”€â”€ parse_chunk/
â”‚   â”‚   â”œâ”€â”€ __init__.py                   # parse_chunk package initializer
â”‚   â”‚   â”œâ”€â”€ doc_docx_to_pdf.py            # Converts .doc/.docx to PDF (LibreOffice headless flow)
â”‚   â”‚   â”œâ”€â”€ formats/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py               # Format module initializer
â”‚   â”‚   â”‚   â”œâ”€â”€ csv.py                    # CSV reader & chunker logic
â”‚   â”‚   â”‚   â”œâ”€â”€ html.py                   # HTML -> Markdown chunker and DOM processing
â”‚   â”‚   â”‚   â”œâ”€â”€ json.py                   # JSON/JSONL flattening and chunking routines
â”‚   â”‚   â”‚   â”œâ”€â”€ md.py                     # Markdown chunking and normalization
â”‚   â”‚   â”‚   â”œâ”€â”€ mp3.py                    # Audio preprocessing wrapper (slicing metadata)
â”‚   â”‚   â”‚   â”œâ”€â”€ pdf.py                    # PDF page extraction and layout-aware parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ png_jpeg_jpg.py           # Image OCR pipeline wrapper
â”‚   â”‚   â”‚   â”œâ”€â”€ ppt_pptx.py               # PPTX slide extractor (n slide = 1 chunk)
â”‚   â”‚   â”‚   â”œâ”€â”€ spreadsheets.py           # Spreadsheet row/column chunking logic
â”‚   â”‚   â”‚   â””â”€â”€ txt.py                    # Plaintext chunkers (paragraph/sentence/window)
â”‚   â”‚   â””â”€â”€ router.py                     # Dispatcher to select parser based on MIME/extension
â”‚   â”œâ”€â”€ relik.sh                          # Helper script to run ReLiK entity/triplet extraction
â”‚   â””â”€â”€ requirements-cpu.txt              # Indexing pipeline runtime dependencies (CPU)
â”‚
â”œâ”€â”€ inference_pipeline/
â”‚   â”œâ”€â”€ Dockerfile                        # Dockerfile for inference server image
â”‚   â”œâ”€â”€ auth_control.py                   # Authentication, authorization middleware and rate limiting for APIs  # observe: logs, metrics
â”‚   â”œâ”€â”€ eval.py                           # RAGAI-Catalyst coherence checks, hit@K monitoring, hallucination detection  # observe: logs, metrics
â”‚   â”œâ”€â”€ frontend/
â”‚   â”‚   â”œâ”€â”€ Dockerfile                    # Frontend container build file
â”‚   â”‚   â”œâ”€â”€ main.py                       # Frontend app entry (UI endpoints / static server)  # observe: logs, metrics
â”‚   â”‚   â”œâ”€â”€ modules                        # Modular UI components / assets
â”‚   â”‚   â””â”€â”€ requirements-cpu.txt          # Frontend Python dependencies
â”‚   â”œâ”€â”€ main.py                           # Inference service entrypoint (REST/gRPC server)  # observe: logs, metrics, traces
â”‚   â”œâ”€â”€ llm_retrieval.py                  # Retrieval orchestration (hybrid BM25 + vector + graph + GeAR lightweight multihop)  # observe: logs, metrics
â”‚   â”œâ”€â”€ llmless_retrieval.py              # Returns only raw chunks/triplets and source urls from arangodb if rate limit exceeded
â”‚   â””â”€â”€ trace_file.py                     # View or download Presigned urls for the raw docs as source link s3://<bucket_name>data/raw/<file_name>.<format>
|
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ eks-manifests/                        
â”‚   â”‚   â”œâ”€â”€ Chart.yaml                    # Helm chart metadata: name, version
â”‚   â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”‚   â”œâ”€â”€ argocd.yaml               # argocd app for orchestrating workloads in order
â”‚   â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ namespaces.yaml       # Define dev/prod namespaces
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ quotas_pdbs.yaml      # ResourceQuotas & PodDisruptionBudgets per namespace
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ serviceaccounts.yaml   # Core service accounts for workloads & operators
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rbac.yaml             # ClusterRole, Role, and bindings for access control
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ network.yaml       # Traefik ingress, NetworkPolicies, security rules
â”‚   â”‚   â”‚   â”œâ”€â”€ dbs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ arangodb.yaml      # ArangoDB StatefulSet, persistence, and resources
â”‚   â”‚   â”‚   â”œâ”€â”€ observability/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ promotheus.yaml    # Prometheus server, alerting rules, and metrics scraping
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ otel_jaeger.yaml   # OpenTelemetry collector & Jaeger tracing setup
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ loki.yaml          # Loki logging backend for cluster logs
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ grafana.yaml       # Grafana dashboard, persistence, and datasource setup
â”‚   â”‚   â”‚   â””â”€â”€ workloads/
â”‚   â”‚   â”‚       â”œâ”€â”€ rayjob_cronjob.yaml # CronJobs for DB backup, Ray indexing pipeline, etc.
â”‚   â”‚   â”‚       â”œâ”€â”€ rayservice.yaml    # RayService deployments: frontend, embedded-reranker, VLLM, valkeye
â”‚   â”‚   â”‚       â””â”€â”€ karpenter-nodepools.yaml # Karpenter provisioners for CPU/GPU autoscaling
â”‚   â”‚   â”œâ”€â”€ values.kind.yaml            # values for local cluster
â”‚   â”‚   â””â”€â”€ values.eks.yaml             # values for prod eks cluster
â”‚   â”‚
â”‚   â”œâ”€â”€ pulumi-aws/                            
â”‚   â”‚   â”œâ”€â”€ config.py                 # Global variables & Pulumi config
â”‚   â”‚   â”œâ”€â”€ vpc.py                    # Networking must exist before cluster
â”‚   â”‚   â”œâ”€â”€ iam_roles_pre_eks.py      # IAM roles required to create EKS
â”‚   â”‚   â”œâ”€â”€ eks_cluster.py            # EKS cluster depends on VPC + pre-EKS IAM
â”‚   â”‚   â”œâ”€â”€ nodegroups.py             # Nodegroups depend on cluster + IAM
â”‚   â”‚   â”œâ”€â”€ iam_roles_post_eks.py     # IAM roles for workloads (Ray, Karpenter, Valkeye)
â”‚   â”‚   â”œâ”€â”€ karpenter.py              # Karpenter provisioning depends on cluster + nodegroups + IAM
â”‚   â”‚   â”œâ”€â”€ cloudflare.py             # DNS records, depends on cluster endpoint
â”‚   â”‚   â”œâ”€â”€ indexing_ami.py           # Indexing AMIs, depends on cluster/nodegroups
â”‚   â”‚   â”œâ”€â”€ inference_ami.py          # Inference/GPU AMIs, depends on cluster/nodegroups
â”‚   â”‚   â”œâ”€â”€ db_backup.py              # CronJobs or backup jobs, depends on DB running in cluster
â”‚   â”‚   â”œâ”€â”€ ___main__.py                  # Orchestrates imports & execution
â”‚   â”‚   â””â”€â”€ pulumi.yaml                   # Pulumi project manifest for infra code
â”‚   â”‚
â”‚   â”œâ”€â”€ onnx/
â”‚   â”‚   â”œâ”€â”€ Dockerfile                    # ONNX runtime image for CPU inference services
â”‚   â”‚   â”œâ”€â”€ grpc.proto                    # gRPC proto definition for ONNX service
â”‚   â”‚   â”œâ”€â”€ rayserve-embedder-reranker.py # Ray Serve wrapper to run embedder + reranker  # observe: logs, metrics
â”‚   â”‚   â””â”€â”€ requirements-cpu.txt          # ONNX service dependencies
â”‚   â”‚
â”‚   â””â”€â”€ vllm/
â”‚       â”œâ”€â”€ Dockerfile                    # GPU-enabled image for vllm serving
â”‚       â”œâ”€â”€ grpc.proto                    # gRPC proto definition for vllm service
â”‚       â”œâ”€â”€ rayserve-vllm.py              # Ray Serve wrapper for vllm inference  # observe: logs, metrics
â”‚       â””â”€â”€ requirements-gpu.txt          # GPU runtime dependencies (CUDA/pytorch/etc.)
|
â””â”€â”€ utils/                                   
|    â”œâ”€â”€ archive/                           # Files no longer maintained
|    â”œâ”€â”€ bootstrap-dev.sh                   # Installs all the required tools for development and testing
|    â”œâ”€â”€ bootstrap-prod.sh                  # Installs minimal tools for prod
|    â”œâ”€â”€ force_sync_data_with_s3.py         # sync/force sync the data/ in local fs/ with s3://<bucket_name>/data/
|    â”œâ”€â”€ lc.sh                              # Local kind cluster for testing rag8s
|    â””â”€â”€ s3_bucket.py                       # Create/delete s3 bucket 
|    
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ build_and_push.sh                   # Builds container images and pushes to registry
â”‚   â”œâ”€â”€ dynamic-values.yaml.sh              # Generates dynamic Helm values (env-specific)
â”‚   â”œâ”€â”€ helm-deploy.sh                      # Wrapper to deploy Helm charts via CI or locally
â”‚   â”œâ”€â”€ pulumi-set-configs.sh               # Sets Pulumi configuration and secrets
â”‚   â””â”€â”€ pulumi-set-secret.sh                # Stores secrets into Pulumi secret store
â”‚
â”œâ”€â”€ .devcontainer/
â”‚   â”œâ”€â”€ Dockerfile                          # Devcontainer image build for local development environment
â”‚   â”œâ”€â”€ devcontainer.json                   # VS Code devcontainer configuration (mounts, settings)
â”‚   â””â”€â”€ scripts
â”‚       â””â”€â”€ fix-docker-group.sh             # Script to fix Docker group permissions inside devcontainer
â”‚
â”œâ”€â”€ .dockerignore                           # Files/dirs excluded from Docker build context
â”œâ”€â”€ .gitignore                              # Git ignore rules
â”œâ”€â”€ Makefile                                # Convenience targets for build/test/deploy tasks
â”œâ”€â”€ README.md                               # Project overview, setup and usage instructions
â””â”€â”€ backups/                                # Local directory that syncs with s3://<bucket_name>/backups
    â””â”€â”€ dbs/
        â””â”€â”€ arrangodb/

```


# Models overview

---

| Model                        | Language     | Params | Max Tokens              | Efficiency           | Triplet Î¼-F1 | size (FP32)  | size (INT8 / W4A16) | RAM needed (INT8) | VRAM needed (W4A16) |
| ---------------------------- | ------------ | ------ | ----------------------- | -------------------- | ------------ | ------------ | ------------------- | ----------------- | ------------------- |
| gte-modernbert-base          | English      | 149M   | 8,192                   | High (CPU/ONNX)      | â€”            | 596 MiB      | 149 MiB (INT8)      | 1.0 GiB           | â€”                   |
| gte-reranker-modernbert-base | English      | 149M   | 8,192                   | Very high (CPU/ONNX) | â€”            | 598 MiB      | 149 MiB (INT8)      | 1.0 GiB           | â€”                   |
| relik-cie-tiny               | English      | 174M   | \~3,000                 | High (CPU)           | 73.8%        | 663.8 MiB    | 165.9 MiB (INT8)    | 0.94 GiB          | â€”                   |
| relik-cie-small              | English      | 216M   | \~3,000                 | high (CPU)           | 74.3%        | 824.0 MiB    | 206.0 MiB (INT8)    | 1.40 GiB          | â€”                   |
| relik-cie-large              | English      | 467M   | \~3,000                 | low (CPU)       | 75.6%        | 1,781.5 MiB  | 445.4 MiB (INT8)    | 2.43 GiB          | â€”                   |
| Qwen3-0.6B-quantized.w4a16   | Multilingual | 600M   | 32,768                  | High (W4A16 AWQ)     | â€”            | \~2.4 GiB\*  | 860 MiB (W4A16)     | â€”                 | \~1.1 GiB           |
| Qwen3-1.7B-quantized.w4a16   | Multilingual | 1.7B   | 32,768                  | High (W4A16 AWQ)     | â€”            | \~6.8 GiB\*  | 2.0 GiB (W4A16)     | â€”                 | \~2.7 GiB           |
| Qwen3-4B-quantized.w4a16     | Multilingual | 4B     | 32,768 (native)         | High (W4A16 AWQ)     | â€”            | \~16.0 GiB\* | 3.43 GiB (W4A16)    | â€”                 | \~5.6 GiB           |
| Qwen3-8B-quantized.w4a16     | Multilingual | 8.2B   | 32,768 / 131,072 (YaRN) | High (W4A16 AWQ)     | â€”            | \~32.8 GiB\* | \~6.5 GiB (W4A16)â€   | â€”                 | \~11 GiB            |
| Qwen3-14B-quantized.w4a16    | Multilingual | 14.8B  | 32,768 / 131,072 (YaRN) | High (W4A16 AWQ)     | â€”            | \~59.2 GiB\* | \~10.9 GiB (W4A16)â€  | â€”                 | \~18 GiB            |

---

## ğŸ”— **References & specialties of the default models in RAG8s**

---

### ğŸ”¹ **\[1] gte-modernbert-base**

* Embedding-only model for dense retrieval in RAG pipelines
* Long-context support: up to **8192 tokens** (Sufficient for page wise chunking)
* Based on **ModernBERT** (FlashAttention 2, RoPE, no position embeddings)
* Optimized for ONNX export and CPU-inference
* Embedding dimension: **768**
* Parameter size: **149M**

ğŸ”— [https://huggingface.co/Alibaba-NLP/gte-modernbert-base](https://huggingface.co/Alibaba-NLP/gte-modernbert-base)

---

### ğŸ”¹ **\[2] gte-reranker-modernbert-base**

* **Cross-encoder reranker** for re-ranking retrieved docs
* High BEIR benchmark score (**nDCG\@10 â‰ˆ 90.7%**)
* Same architecture & size as embedding model (149M), supports **8192 tokens**
* Extremely fast CPU inference with ONNX (FlashAttention 2)

ğŸ”— [https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base](https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base)

---

### ğŸ”¹ **\[3] ReLiK-CIE-tiny(For precomputing triplets, not deployed)**

A compact and efficient **entity + relation extraction** model designed for **Graph-RAG pipelines**. Unlike fast entity-only models (e.g., SpEL, ReFinED), `relik-cie-tiny` can extract both **named entities** and **semantic triplets** (`(head, relation, tail)`), enabling direct construction of **knowledge subgraphs** from raw text.

* Extracts **entities and triplets** in a single pass
* Balanced for **accuracy and runtime performance**

ğŸ”— [relik-ie/relik-cie-tiny](https://huggingface.co/relik-ie/relik-cie-tiny)

---

### ğŸ”¹ **Qwen3-4B-W4A16 (vLLM Deployment)**

A compact, high-throughput **instruction-tuned LLM** quantized with **W4A16** (4-bit weights + FP16 activations). Built on **Qwen3-4B**, this variant supports **32,768-token context** natively and achieves performance comparable to models 10Ã— its size (e.g., Qwen2.5-72B). Optimized for vLLM inference, it balances speed, memory efficiency, and accuracy, running efficiently on GPUs like A10G, L4, and L40S. **vLLM fully supports W4A16, awq and MoE models like Qwen3-30B-A3B, leveraging CUDA kernels for faster inference (sglang doesnt support these yet) and horizontal scaling**.

* Architecture: **Transformer** (Qwen3 series, multilingual)
* Context Length: **32k tokens** (vLLM-native)
* Quantization: **W4A16 (AWQ)** â€” 4-bit weights, FP16 activations
* VRAM Usage: **â‰ˆ4.8â€“5.2 GiB** (fits on 24 GiB GPUs with headroom)

ğŸ”— [RedHatAI Qwen3-4B-W4A16](https://huggingface.co/RedHatAI/Qwen3-4B-quantized.w4a16)

> â€œEven a tiny model like Qwen3-4B can rival the performance of Qwen2.5-72B-Instruct.â€
â€” [Qwen3 Blog](https://qwenlm.github.io/blog/qwen3/#introduction) , [Thinking-mode](https://qwenlm.github.io/blog/qwen3/#key-features)

---



<details>
 <summary>EC2 (Click the triangle)</summary>

## EC2 instances for spot with fallback pre warmed on-demand scaling with karpenter-gpu

#### **G6f (Fractional L4 GPU) Sizes**

| Instance        | vCPUs | Memory  | GPU Fraction | GPU Memory |
| --------------- | ----- | ------- | ------------ | ---------- |
| **g6f.large**   | 2     | 8 GiB   | 0.125 Ã— L4   | \~3 GiB    |
| **g6f.xlarge**  | 4     | 16 GiB  | 0.25 Ã— L4    | \~6 GiB    |
| **g6f.2xlarge** | 8     | 32 GiB  | 0.5 Ã— L4     | \~12 GiB   |
| **g6f.4xlarge** | 16    | 64 GiB  | 1 Ã— L4       | 24 GiB     |
| **g6f.8xlarge** | 32    | 128 GiB | 2 Ã— L4       | 48 GiB     |

---

#### **G6e (Full L40S GPU) Sizes**

| Instance         | vCPUs | Memory   | GPUs | GPU Memory                               |
| ---------------- | ----- | -------- | ---- | ---------------------------------------- |
| **g6e.xlarge**   | 4     | 32 GiB   | 1    | 44 GiB (1Ã—44)   ([AWS Documentation][1]) |
| **g6e.2xlarge**  | 8     | 64 GiB   | 1    | 44 GiB (1Ã—44)   ([AWS Documentation][1]) |
| **g6e.4xlarge**  | 16    | 128 GiB  | 1    | 44 GiB (1Ã—44)   ([AWS Documentation][1]) |
| **g6e.8xlarge**  | 32    | 256 GiB  | 1    | 44 GiB (1Ã—44)   ([AWS Documentation][1]) |
| **g6e.12xlarge** | 48    | 384 GiB  | 4    | 178 GiB (4Ã—44)  ([AWS Documentation][1]) |
| **g6e.16xlarge** | 64    | 512 GiB  | 1    | 44 GiB (1Ã—44)   ([AWS Documentation][1]) |
| **g6e.24xlarge** | 96    | 768 GiB  | 4    | 178 GiB (4Ã—44)  ([AWS Documentation][1]) |
| **g6e.48xlarge** | 192   | 1536 GiB | 8    | 357 GiB (8Ã—44)  ([AWS Documentation][1]) |

---
# Local NVMe based EC2s for hosting vector dbs like qdrant, arrangodb as statefulsets 
| Instance     | vCPU / RAM       | **C8g On-Demand** (USD/hr) | **C8g On-Demand** (â‚¹/hr) | **C8g Spot** (USD/hr)   | **C8g Spot** (â‚¹/hr) | **C8gd On-Demand** (USD/hr) | **C8gd On-Demand** (â‚¹/hr) | **C8gd Spot** (USD/hr)  | **C8gd Spot** (â‚¹/hr) |
| ------------ | ---------------- | -------------------------- | ------------------------ | ----------------------- | ------------------- | --------------------------- | ------------------------- | ----------------------- | -------------------- |
| **medium**   | 1 vCPU/2 GiB     | \$0.044 ([Vantage][1])     | â‚¹3.7                     | \$0.013 ([Vantage][1])  | â‚¹1.1                | \$0.054 ([Vantage][2])      | â‚¹4.5                      | \$0.012 ([Vantage][2])  | â‚¹1.0                 |
| **xlarge**   | 4 vCPU/8 GiB     | \$0.702 ([Vantage][3])     | â‚¹58.2                    | \$0.238 ([Vantage][3])  | â‚¹19.8               | \$0.196 ([Vantage][4])      | â‚¹16.3                     | \$0.080 ([Vantage][4])  | â‚¹6.6                 |
| **2xlarge**  | 8 vCPU/16 GiB    | \$0.319 ([Vantage][5])     | â‚¹26.5                    | \$0.128 ([Vantage][5])  | â‚¹10.6               | â€”                           | â€”                         | â€”                       | â€”                    |
| **4xlarge**  | 16 vCPU/32 GiB   | \$0.702 ([Vantage][3])     | â‚¹58.2                    | \$0.238 ([Vantage][3])  | â‚¹19.8               | \$0.784 ([Vantage][6])      | â‚¹65.1                     | \$0.298 ([Vantage][6])  | â‚¹24.7                |
| **8xlarge**  | 32 vCPU/64 GiB   | \$1.276 ([Vantage][7])     | â‚¹105.9                   | \$0.522 ([Vantage][7])  | â‚¹43.3               | \$1.720 ([Vantage][8])      | â‚¹142.8                    | \$â€” (spot NA)           | â€”                    |
| **16xlarge** | 64 vCPU/128 GiB  | \$2.808 ([Vantage][9])     | â‚¹233.1                   | \$0.731 ([Vantage][9])  | â‚¹60.7               | \$3.135 ([Vantage][10])     | â‚¹259.9                    | \$0.796 ([Vantage][10]) | â‚¹66.1                |
| **24xlarge** | 96 vCPU/192 GiB  | \$4.211 ([Vantage][8])     | â‚¹349.3                   | \$1.065 ([Vantage][8])  | â‚¹88.4               | \$5.173 ([Vantage][8])      | â‚¹429.4                    | \$1.476 ([Vantage][8])  | â‚¹122.5               |
| **48xlarge** | 192 vCPU/384 GiB | \$7.657 ([Vantage][11])    | â‚¹635.6                   | \$1.979 ([Vantage][11]) | â‚¹164.3              | \$9.406 ([Vantage][12])     | â‚¹780.8                    | \$3.323 ([Vantage][12]) | â‚¹275.6               |

---

### Notes

* â€œâ€”â€ indicates data not available or not listed for that size.
* Spot prices fluctuate; these are **minimums** observed at time of lookup.
* INR values are approximate (USD Ã— 83).
* NVMe-equipped C8gd variants include large local SSD at no extra configuration cost; C8g has no NVMe.
* Source pricing from Instances.Vantage.sh snapshots as of early August 2025.

[1]: https://instances.vantage.sh/aws/ec2/c8g.medium
[2]: https://instances.vantage.sh/aws/ec2/c8gd.medium
[3]: https://instances.vantage.sh/aws/ec2/c8g.4xlarge
[4]: https://instances.vantage.sh/aws/ec2/c8gd.xlarge?cost_duration=monthly&os=linux&region=us-east-1&reserved_term=Standard.noUpfront
[5]: https://instances.vantage.sh/aws/ec2/c8g.2xlarge
[6]: https://instances.vantage.sh/aws/ec2/c8gd.4xlarge
[7]: https://instances.vantage.sh/aws/ec2/c8g.8xlarge
[8]: https://instances.vantage.sh/aws/ec2/c8gd.24xlarge
[9]: https://instances.vantage.sh/aws/ec2/c8g.16xlarge
[10]: https://instances.vantage.sh/aws/ec2/c8gd.16xlarge
[11]: https://instances.vantage.sh/aws/ec2/c8g.48xlarge
[12]: https://instances.vantage.sh/aws/ec2/c8gd.48xlarge

</details>












