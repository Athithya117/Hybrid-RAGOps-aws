
## Research Support

* NVIDIA (mid-2025): Page-level chunking is baseline best https://developer.nvidia.com/blog/finding-the-best-chunking-strategy-for-accurate-ai-responses/





### ğŸ“˜ Information Extraction Models (ReLiK vs. REBEL)

| Model                        | Params (Retriever + Reader) | Entity Linking (Microâ€‘F1) | Relation Extraction (Microâ€‘F1)                                             |
| ---------------------------- | --------------------------- | ------------------------- | -------------------------------------------------------------------------- |
| **relik-ie/relik-cie-small** | 33â€¯M + 141â€¯M = **174â€¯M**    | **94.4â€¯%**                | **75.8â€¯%**                                                                 |
| **relik-ie/relik-cie-large** | 33â€¯M + 434â€¯M = **467â€¯M**    | **95.0â€¯%**                | **78.1â€¯%**                                                                 |
| **Babelscape/rebel-large**   | \~**406â€¯M** (BART-based)    | N/A (no EL)               | **93.4â€¯%** (NYT RE) <br> **76.65â€¯%** (CoNLL04 RE) <br> **82.2â€¯%** (ADE RE) |

---

### ğŸ“— Embedding Models (Alibaba GTE/mGTE)

| Model                     | Params  | Embedding Dim | Max Seq Length | MTEB Score | LoCo Score | COIR Score |
| ------------------------- | ------- | ------------- | -------------- | ---------- | ---------- | ---------- |
| **gte-modernbert-base**   | 149â€¯M   | 768           | 8192 tokens    | **64.38**  | **87.57**  | **79.31**  |
| **gte-base-en-v1.5**      | 137â€¯M   | 768           | 8192 tokens    | **64.11**  | **87.44**  | â€”          |
| **gte-multilingual-base** | \~300â€¯M | 768           | 8192 tokens    | â€”          | â€”          | â€”          |
| **gte-large-en-v1.5**     | 434â€¯M   | 1024          | 8192 tokens    | **65.39**  | **86.71**  | â€”          |

---

### ğŸ” Final Suggestion (Prod-Ready)

*  **relik-ie/relik-cie-small**: Best **entity + relation** extraction model for scale with high accuracy.
*  **gte-modernbert-base**: Best tradeoff for **embedding quality**, long context, and low latency.

These two together form a compact but accurate pipeline suitable for **graph+vector-based RAG** at production scale.




