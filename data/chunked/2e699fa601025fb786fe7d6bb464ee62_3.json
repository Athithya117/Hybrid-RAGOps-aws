{
  "document_id": "2e699fa601025fb786fe7d6bb464ee62",
  "chunk_id": "2e699fa601025fb786fe7d6bb464ee62_3",
  "chunk_type": "token_window",
  "text": "AGBattlePacket**: A collection of tax consulting PDF reports from Deloitte. ### Evaluation methodology We established a comprehensive evaluation framework to systematically compare chunking strategies across different datasets and measure their impact on RAG system performance. **Primary metric**: End-to-end RAG answer accuracy**Evaluation process**: Multiple trials per configuration with many judge models ### Evaluation metrics For our evaluation, we used the NV Answer Accuracy metric from the RAGAS evaluation framework, which measures the agreement between a model’s response and a reference ground truth for a given question. The Answer Accuracy metric works by: - Using LLM-as-a-judge to evaluate the correctness of generated responses. - Comparing the model’s outputs against ground-truth references. - Scoring on a scale of 0-4, where: **0**: The response is inaccurate or doesn’t address the question.**2**: The response partially aligns with the reference.**4**: The response exactly aligns with the reference. To ensure robustness, each evaluation involves multiple judgment runs with different judge models, and the scores are averaged to produce the final accuracy metric. For our experiments, we used the following powerful models as judges: **Mixtral 8x22B Instruct**(mistralai/mixtral-8x22b-instruct-v0.1)**Llama 3.1 70B Instruct**(meta/llama-3.1-70b-instruct) Using these large language models as judges provided high-quality assessments, while the multi-judge approach (a.k.a. “council of judges”) helped minimize bias from any single evaluator model, resulting in more reliable performance measurements across different chunking strategies. **Ingestion framework** For our experiments, we used two different document ingestion frameworks to ensure fair comparisons between different chunking strategies: **NVIDIA NeMo Retriever**: Used for extracting content for page-level and token-based chunking strategies. This set of microservices is designed for parsing complex, unstructured PDFs and other enterprise documents, enabling us to:- Extract high-quality text while preserving document structure. - Capture tables and charts from financial reports and technical documents. - Process hundreds of documents efficiently across our diverse datasets. **nemoretriever-parse**: Model used specifically for section-level chunking, as it can intelligently detect section headers and document structure. For a fair comparison between page-level and section-level chunking, we also used nemoretriever-parse",
  "token_count": 500,
  "embedding": null,
  "file_type": "text/html",
  "source_url": "s3://e2e-rag-system-42/data/raw/htmls/best_chunking_stratergy.html",
  "page_number": null,
  "slide_range": null,
  "row_range": null,
  "token_range": [
    900,
    1400
  ],
  "audio_range": null,
  "timestamp": "2025-09-17T12:50:02Z",
  "parser_version": "trafilatura-only-v2",
  "tags": [],
  "layout_tags": [
    "page"
  ],
  "used_ocr": false,
  "parse_chunk_duration_ms": 429,
  "heading_path": [],
  "headings": [
    "Finding the Best Chunking Strategy for Accurate AI Responses | NVIDIA Technical Blog"
  ],
  "line_range": null,
  "chunk_duration_ms": null,
  "snapshot_url": null
}