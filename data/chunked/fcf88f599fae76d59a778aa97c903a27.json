[
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p1_0",
    "chunk_type": "pdf_page_chunk",
    "text": "arXiv:2305.03393v1 [cs. CV] 5 May 2023 1 Introduction Optimized Table Tokenization for Table Structure Recognition Maksym Lysak [0000 − 0002 − 3723 − 6960] , Ahmed Nassar [0000 − 0002 − 9468 − 0822] , Nikolaos Livathinos [0000 − 0001 − 8513 − 3491] , Christoph Auer [0000 − 0001 − 5761 − 0422] , and Peter Staar [0000 − 0002 − 8088 − 0823] IBM Research {mly,ahn,nli,cau,taa}@zurich.ibm.com Abstract. Extracting tables from documents is a crucial task in any document conversion pipeline. Recently, transformer-based models have demonstrated that table-structure can be recognized with impressive ac- curacy using Image-to-Markup-Sequence (Im2Seq) approaches. Taking only the image of a table, such models predict a sequence of tokens (e.g. in HTML, LaTeX) which represent the structure of the table. Since the token representation of the table structure has a signiﬁcant impact on the accuracy and run-time performance of any Im2Seq model, we inves- tigate in this paper how table-structure representation can be optimised. We propose a new, optimised table-structure language (OTSL) with a minimized vocabulary and speciﬁc rules. The beneﬁts of OTSL are that it reduces the number of tokens to 5 (HTML needs 28+) and shortens the sequence length to half of HTML on average. Consequently, model accuracy improves signiﬁcantly, inference time is halved compared to HTML-based models, and the predicted table structures are always syn- tactically correct. This in turn eliminates most post-processing needs. Popular table structure data-sets will be published in OTSL format to the community. Keywords: Table Structure Recognition · Data Representation · Trans- formers · Optimization. Tables are ubiquitous in documents such as scientiﬁc papers, patents, reports, manuals, speciﬁcation sheets or marketing material. They often encode highly valuable information and therefore need to be extracted with high accuracy.",
    "token_count": 494,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 1,
    "timestamp": "2025-09-27T16:43:34.678015Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p1_1",
    "chunk_type": "pdf_page_chunk",
    "text": "Tables are ubiquitous in documents such as scientiﬁc papers, patents, reports, manuals, speciﬁcation sheets or marketing material. They often encode highly valuable information and therefore need to be extracted with high accuracy. Unfortunately, tables appear in documents in various sizes, styling and struc- ture, making it diﬃcult to recover their correct structure with simple analyt- ical methods. Therefore, accurate table extraction is achieved these days with machine-learning based methods. In modern document understanding systems [1,15], table extraction is typi- cally a two-step process. Firstly, every table on a page is located with a bounding box, and secondly, their logical row and column structure is recognized. As of Firstly, every table on a page is located with a bounding box, and secondly, their logical row and column structure is recognized. As of As of",
    "token_count": 188,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 1,
    "timestamp": "2025-09-27T16:43:34.678522Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p2_0",
    "chunk_type": "pdf_page_chunk",
    "text": "2 M. Lysak, et al. C HTML sequence length: 55vocabulary for this table: 12 tokens Fig. 1. Comparison between HTML and OTSL table structure representation: (A) table-example with complex row and column headers, including a 2D empty span, (B) minimal graphical representation of table structure using rectangular layout, (C) HTML representation, (D) OTSL representation. This example demonstrates many of the key-features of OTSL, namely its reduced vocabulary size (12 versus 5 in this case), its reduced sequence length (55 versus 30) and a enhanced internal structure (variable token sequence length per row in HTML versus a ﬁxed length of rows in OTSL).today, table detection in documents is a well understood problem, and the latest state-of-the-art (SOTA) object detection methods provide an accuracy compa- rable to human observers [7,8,10,14,23]. On the other hand, the problem of table structure recognition (TSR) is a lot more challenging and remains a very active area of research, in which many novel machine learning algorithms are being explored [3,4,5,9,11,12,13,14,17,18,21,22]. Recently emerging SOTA methods for table structure recognition employ transformer-based models, in which an image of the table is provided to the net- work in order to predict the structure of the table as a sequence of tokens. These image-to-sequence (Im2Seq) models are extremely powerful, since they allow for a purely data-driven solution. The tokens of the sequence typically belong to a markup language such as HTML, Latex or Markdown, which allow to describe table structure as rows, columns and spanning cells in various conﬁgurations. In Figure 1, we illustrate how HTML is used to represent the table-structure of a particular example table. Public table-structure data sets such as PubTab- Net [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR. D OTSL sequence length: 30vocabulary: 5 tokens NL Public table-structure data sets such as PubTab- Net [22], and FinTabNet [21], which were created in a semi-automated way from paired PDF and HTML sources (e.g. PubMed Central), popularized primarily the use of HTML as ground-truth representation format for TSR. D OTSL sequence length: 30vocabulary: 5 tokens NL D OTSL sequence length: 30vocabulary: 5 tokens NL",
    "token_count": 576,
    "figures": "[\"\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\", \"<table>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t<tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t<td\\tcolspan=\\u201c2\\u201d\\t\\trowspan=\\u201c2\\u201d\\t\\t\\t\\t>\\t</td>\\t\\t\\t<td\\tcolspan=\\u201c3\\u201d\\t\\t\\t>\\t</td>\\t\\t\\n\\t</tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t<tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t<td\\t></td>\\t<td>\\t</td>\\t\\t<td>\\t</td>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t</tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t<tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t<td\\trowspan=\\u201c3\\t\\t\\u201d>\\t</td><\\t\\ttd>\\t</td>\\t\\t\\t<td>\\t\\t</td>\\t<td>\\t</td>\\t\\t\\t<td>\\t</td>\\n\\t</tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t<tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t<td\\t></td>\\t<td>\\t</td>\\t\\t<td>\\t</td>\\t\\t\\t<td>\\t\\t</td>\\t\\t\\t\\t\\t\\t\\t\\n\\t</tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t<tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\t\\t<td\\t></td>\\t<td>\\t</td>\\t\\t<td>\\t</td>\\t\\t\\t<td>\\t\\t</td>\\t\\t\\t\\t\\t\\t\\t\\n\\t</tr>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n</table>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\", \"C\\tL\\tC\\tL\\tL\\nU\\tX\\tC\\tC\\tC\\nC\\tC\\tC\\tC\\tC\\nU\\tC\\tC\\tC\\tC\\nU\\tC\\tC\\tC\\tC\", \"<table>\\t<tr>\\t</tr>\\t<td>\\t</td>\\t\\t<td\\tcolspan=\\\"2\\\"\\tcolspan=\\\"3\\\"\\nrowspan=\\\"2\\\"\\t\\trowspan=\\\"3\\\"\\t\\t\\t> <\\t/table>\\t\\t\", \"C\\tL\\tU\\tX\", \"Observer 1 benign observer 2 berign 13 15 Observer 2 mafignant O 62 62 Total observer 1 13 64 77\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 2,
    "timestamp": "2025-09-27T16:43:37.694762Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p3_0",
    "chunk_type": "pdf_page_chunk",
    "text": "2 Related Work Optimized Table Tokenization for Table Structure Recognition 3 While the majority of research in TSR is currently focused on the develop- ment and application of novel neural model architectures, the table structure representation language (e.g. HTML in PubTabNet and FinTabNet) is usually adopted as is for the sequence tokenization in Im2Seq models. In this paper, we aim for the opposite and investigate the impact of the table structure rep- resentation language with an otherwise unmodiﬁed Im2Seq transformer-based architecture. Since the current state-of-the-art Im2Seq model is TableFormer [9], we select this model to perform our experiments. The main contribution of this paper is the introduction of a new optimised ta- ble structure language (OTSL), speciﬁcally designed to describe table-structure in an compact and structured way for Im2Seq models. OTSL has a number of key features, which make it very attractive to use in Im2Seq models. Speciﬁcally, compared to other languages such as HTML, OTSL has a minimized vocabulary which yields short sequence length, strong inherent structure (e.g. strict rectan- gular layout) and a strict syntax with rules that only look backwards. The latter allows for syntax validation during inference and ensures a syntactically correct table-structure. These OTSL features are illustrated in Figure 1, in comparison to HTML. The paper is structured as follows. In section 2, we give an overview of the latest developments in table-structure reconstruction. In section 3 we review the current HTML table encoding (popularised by PubTabNet and FinTabNet) and discuss its ﬂaws. Subsequently, we introduce OTSL in section 4, which in- cludes the language deﬁnition, syntax rules and error-correction procedures. In section 5, we apply OTSL on the TableFormer architecture, compare it to Table- Former models trained on HTML and ultimately demonstrate the advantages of using OTSL. Finally, in section 6 we conclude our work and outline next potential steps. Approaches to formalize the logical structure and layout of tables in electronic documents date back more than two decades [16].",
    "token_count": 478,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 3,
    "timestamp": "2025-09-27T16:43:37.995027Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p3_1",
    "chunk_type": "pdf_page_chunk",
    "text": "Finally, in section 6 we conclude our work and outline next potential steps. Approaches to formalize the logical structure and layout of tables in electronic documents date back more than two decades [16]. In the recent past, a wide variety of computer vision methods have been explored to tackle the prob- lem of table structure recognition, i.e. the correct identiﬁcation of columns, rows and spanning cells in a given table. Broadly speaking, the current deep- learning based approaches fall into three categories: object detection (OD) meth- ods, Graph-Neural-Network (GNN) methods and Image-to-Markup-Sequence (Im2Seq) methods. Object-detection based methods [11,12,13,14,21] rely on table- structure annotation using (overlapping) bounding boxes for training, and pro- duce bounding-box predictions to deﬁne table cells, rows, and columns on a table image. Graph Neural Network (GNN) based methods [3,6,17,18], as the name suggests, represent tables as graph structures. The graph nodes represent the content of each table cell, an embedding vector from the table image, or geomet- ric coordinates of the table cell. The edges of the graph deﬁne the relationship between the nodes, e.g. if they belong to the same column, row, or table cell. The graph nodes represent the content of each table cell, an embedding vector from the table image, or geomet- ric coordinates of the table cell. The edges of the graph deﬁne the relationship between the nodes, e.g. if they belong to the same column, row, or table cell. The edges of the graph deﬁne the relationship between the nodes, e.g. if they belong to the same column, row, or table cell.",
    "token_count": 402,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 3,
    "timestamp": "2025-09-27T16:43:37.995553Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p4_0",
    "chunk_type": "pdf_page_chunk",
    "text": "4 M. Lysak, et al.3 Problem Statement Other work [20] aims at predicting a grid for each table and deciding which cells must be merged using an attention network. Im2Seq methods cast the problem as a sequence generation task [4,5,9,22], and therefore need an internal table- structure representation language, which is often implemented with standard markup languages (e.g. HTML, LaTeX, Markdown). In theory, Im2Seq methods have a natural advantage over the OD and GNN methods by virtue of directly predicting the table-structure. As such, no post-processing or rules are needed in order to obtain the table-structure, which is necessary with OD and GNN approaches. In practice, this is not entirely true, because a predicted sequence of table-structure markup does not necessarily have to be syntactically correct. Hence, depending on the quality of the predicted sequence, some post-processing needs to be performed to ensure a syntactically valid (let alone correct) sequence. Within the Im2Seq method, we ﬁnd several popular models, namely the encoder-dual-decoder model (EDD) [22], TableFormer [9], Tabsplitter[2] and Ye et. al. [ 19]. EDD uses two consecutive long short-term memory (LSTM) decoders to predict a table in HTML representation. The tag decoder predicts a sequence of HTML tags. For each decoded table cell ( <td> ), the attention is passed to the cell decoder to predict the content with an embedded OCR approach. The latter makes it susceptible to transcription errors in the cell content of the table. TableFormer address this reliance on OCR and uses two transformer decoders for HTML structure and cell bounding box prediction in an end-to-end architecture. The predicted cell bounding box is then used to extract text tokens from an originating (digital) PDF page, circumventing any need for OCR. TabSplitter [2] proposes a compact double-matrix representation of table rows and columns to do error detection and error correction of HTML structure sequences based on predictions from [19]. This compact double-matrix representation can not be used directly by the Img2seq model training, so the model uses HTML as an intermediate form. Chi et. al. [",
    "token_count": 494,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 4,
    "timestamp": "2025-09-27T16:43:38.265930Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p4_1",
    "chunk_type": "pdf_page_chunk",
    "text": "Chi et. al. [ 4] introduce a data set and a baseline method using bidirectional LSTMs to predict LaTeX code. Kayal [5] introduces Gated ResNet transformers to predict LaTeX code, and a separate OCR module to extract content. Im2Seq approaches have shown to be well-suited for the TSR task and allow a full end-to-end network design that can output the ﬁnal table structure without pre- or post-processing logic. Furthermore, Im2Seq models have demonstrated to deliver state-of-the-art prediction accuracy [9]. This motivated the authors to investigate if the performance (both in accuracy and inference time) can be further improved by optimising the table structure representation language. We believe this is a necessary step before further improving neural network architectures for this task. All known Im2Seq based models for TSR fundamentally work in similar ways. Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a ﬁnite vocab- Given an image of a table, the Im2Seq model predicts the structure of the table by generating a sequence of tokens. These tokens originate from a ﬁnite vocab- These tokens originate from a ﬁnite vocab-",
    "token_count": 285,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 4,
    "timestamp": "2025-09-27T16:43:38.266405Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p5_0",
    "chunk_type": "pdf_page_chunk",
    "text": "ulary and can be interpreted as a table structure. For example, with the HTML tokens <table> , </table> , <tr> , </tr> , <td> and </td> , one can construct simple table structures without any spanning cells. In reality though, one needs at least 28 HTML tokens to describe the most common complex tables observed in real-world documents [21,22], due to a variety of spanning cells deﬁnitions in the HTML token vocabulary. Fig. 2. Frequency of tokens in HTML and OTSL as they appear in PubTabNet. Obviously, HTML and other general-purpose markup languages were not de- signed for Im2Seq models. As such, they have some serious drawbacks. First, the token vocabulary needs to be artiﬁcially large in order to describe all plausible tabular structures. Since most Im2Seq models use an autoregressive approach, they generate the sequence token by token. Therefore, to reduce inference time, a shorter sequence length is critical. Every table-cell is represented by at least two tokens ( <td> and </td> ). Furthermore, when tokenizing the HTML struc- ture, one needs to explicitly enumerate possible column-spans and row-spans as words. In practice, this ends up requiring 28 diﬀerent HTML tokens (when including column- and row-spans up to 10 cells) just to describe every table in the PubTabNet dataset. Clearly, not every token is equally represented, as is depicted in Figure 2. This skewed distribution of tokens in combination with variable token row-length makes it challenging for models to learn the HTML structure. Additionally, it would be desirable if the representation would easily allow an early detection of invalid sequences on-the-go, before the prediction of the entire table structure is completed. HTML is not well-suited for this purpose as the veriﬁcation of incomplete sequences is non-trivial or even impossible. In a valid HTML table, the token sequence must describe a 2D grid of table cells, serialised in row-major ordering, where each row and each column have the same length (while considering row- and column-spans). Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierar- chical manner.",
    "token_count": 488,
    "figures": "[\"HTML OTSL 1E+08 1E+06 1E+04 1E+02 1E+00 G C NL L U X <td </td> <thead> <tbody> </thead> </tbody> colspan=\\\" colspan=\\\"3\\\" colspan=\\\"5\\\" colspan=\\\" colspan=\\\"' rowspan=\\\"7\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 5,
    "timestamp": "2025-09-27T16:43:43.118342Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p5_1",
    "chunk_type": "pdf_page_chunk",
    "text": "In a valid HTML table, the token sequence must describe a 2D grid of table cells, serialised in row-major ordering, where each row and each column have the same length (while considering row- and column-spans). Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierar- chical manner. Since the number of tokens for each table row and column can vary signiﬁcantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence Optimized Table Tokenization for Table Structure Recognition 5 Furthermore, every opening tag in HTML needs to be matched by a closing tag in a correct hierar- chical manner. Since the number of tokens for each table row and column can vary signiﬁcantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence Optimized Table Tokenization for Table Structure Recognition 5 Since the number of tokens for each table row and column can vary signiﬁcantly, especially for large tables with many row- and column-spans, it is complex to verify the consistency of predicted structures during sequence Optimized Table Tokenization for Table Structure Recognition 5",
    "token_count": 272,
    "figures": "[\"HTML OTSL 1E+08 1E+06 1E+04 1E+02 1E+00 G C NL L U X <td </td> <thead> <tbody> </thead> </tbody> colspan=\\\" colspan=\\\"3\\\" colspan=\\\"5\\\" colspan=\\\" colspan=\\\"' rowspan=\\\"7\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 5,
    "timestamp": "2025-09-27T16:43:43.119085Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p6_0",
    "chunk_type": "pdf_page_chunk",
    "text": "6 M. Lysak, et al. 4.1 Language Deﬁnition 4 Optimised Table Structure Language generation. Implicitly, this also means that Im2Seq models need to learn these complex syntax rules, simply to deliver valid output. In practice, we observe two major issues with prediction quality when train- ing Im2Seq models on HTML table structure generation from images. On the one hand, we ﬁnd that on large tables, the visual attention of the model often starts to drift and is not accurately moving forward cell by cell anymore. This manifests itself in either in an increasing location drift for proposed table-cells in later rows on the same column or even complete loss of vertical alignment, as illustrated in Figure 5. Addressing this with post-processing is partially possible, but clearly undesired. On the other hand, we ﬁnd many instances of predictions with structural inconsistencies or plain invalid HTML output, as shown in Fig- ure 6, which are nearly impossible to properly correct. Both problems seriously impact the TSR model performance, since they reﬂect not only in the task of pure structure recognition but also in the equally crucial recognition or matching of table cell content. To mitigate the issues with HTML in Im2Seq-based TSR models laid out before, we propose here our Optimised Table Structure Language (OTSL). OTSL is designed to express table structure with a minimized vocabulary and a simple set of rules, which are both signiﬁcantly reduced compared to HTML. At the same time, OTSL enables easy error detection and correction during sequence generation. We further demonstrate how the compact structure representation and minimized sequence length improves prediction accuracy and inference time in the TableFormer architecture. In Figure 3, we illustrate how the OTSL is deﬁned. In essence, the OTSL deﬁnes only 5 tokens that directly describe a tabular structure based on an atomic 2D grid.",
    "token_count": 407,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 6,
    "timestamp": "2025-09-27T16:43:43.379715Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p6_1",
    "chunk_type": "pdf_page_chunk",
    "text": "In Figure 3, we illustrate how the OTSL is deﬁned. In essence, the OTSL deﬁnes only 5 tokens that directly describe a tabular structure based on an atomic 2D grid. The OTSL vocabulary is comprised of the following tokens: – \"C\" cell - a new table cell that either has or does not have cell content – \"L\" cell - left-looking cell , merging with the left neighbor cell to create a span – \"U\" cell - up-looking cell , merging with the upper neighbor cell to create a span – \"X\" cell - cross cell , to merge with both left and upper neighbor cells – \"NL\" - new-line , switch to the next row. A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML.",
    "token_count": 175,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 6,
    "timestamp": "2025-09-27T16:43:43.380297Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p6_2",
    "chunk_type": "pdf_page_chunk",
    "text": "The OTSL vocabulary is comprised of the following tokens: – \"C\" cell - a new table cell that either has or does not have cell content – \"L\" cell - left-looking cell , merging with the left neighbor cell to create a span – \"U\" cell - up-looking cell , merging with the upper neighbor cell to create a span – \"X\" cell - cross cell , to merge with both left and upper neighbor cells – \"NL\" - new-line , switch to the next row. A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML. A notable attribute of OTSL is that it has the capability of achieving lossless conversion to HTML.",
    "token_count": 148,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 6,
    "timestamp": "2025-09-27T16:43:43.380610Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p7_0",
    "chunk_type": "pdf_page_chunk",
    "text": "B E 1 2 4.2 Language Syntax The OTSL representation follows these syntax rules: Fig. 3. OTSL description of table structure: A - table example; B - graphical repre- sentation of table structure; C - mapping structure on a grid; D - OTSL structure encoding; E - explanation on cell encoding1. Left-looking cell rule : The left neighbour of an \"L\" cell must be either another \"L\" cell or a \"C\" cell. 2. Up-looking cell rule : The upper neighbour of a \"U\" cell must be either another \"U\" cell or a \"C\" cell. 3. Cross cell rule : The left neighbour of an \"X\" cell must be either another \"X\" cell or a \"U\" cell, and the upper neighbour of an \"X\" cell must be either another \"X\" cell or an \"L\" cell. 4. First row rule : Only \"L\" cells and \"C\" cells are allowed in the ﬁrst row. 5. First column rule : Only \"U\" cells and \"C\" cells are allowed in the ﬁrst column. 6. Rectangular rule : The table representation is always rectangular - all rows must have an equal number of tokens, terminated with \"NL\" token. The application of these rules gives OTSL a set of unique properties. First of all, the OTSL enforces a strictly rectangular structure representation, where every new-line token starts a new row. As a consequence, all rows and all columns have exactly the same number of tokens, irrespective of cell spans. Secondly, the OTSL representation is unambiguous: Every table structure is represented in one way. In this representation every table cell corresponds to a \"C\"-cell token, which in case of spans is always located in the top-left corner of the table cell deﬁnition. Third, OTSL syntax rules are only backward-looking. As a consequence, every predicted token can be validated straight during sequence generation by looking at the previously predicted sequence. As such, OTSL can guarantee that every predicted sequence is syntactically valid. These characteristics can be easily learned by sequence generator networks, as we demonstrate further below.",
    "token_count": 467,
    "figures": "[\"\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\", \"C\\tL\\tC\\tL\\tL\\nU\\tX\\tC\\tC\\tC\\nC\\tC\\tC\\tC\\tC\\nU\\tC\\tC\\tC\\tC\\nU\\tC\\tC\\tC\\tC\", \"\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\", \"C\\nU\\n\\nU\", \"C\\tL\\t\\tL\\tL\\nU\\tX X X\\nX X X\\t\\t\\t\\n\\t\\t\\t\\t\\nU\\t\\t\\t\\t\", \"C\\t\\tL\\tL\\tL\", \"2 [q0 4 Total benig melignant observer 2 3 13 PI 15 malignant 62 62 Total ohserwer 1 13 64\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 7,
    "timestamp": "2025-09-27T16:43:49.156916Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p7_1",
    "chunk_type": "pdf_page_chunk",
    "text": "As such, OTSL can guarantee that every predicted sequence is syntactically valid. These characteristics can be easily learned by sequence generator networks, as we demonstrate further below. We ﬁnd strong indications that this pattern Optimized Table Tokenization for Table Structure Recognition 7 1 - simple cells: \"C\" 2 - horizontal merges: \"C\", \"L\" 3 - vertical merges: \"C\", \"U\" 4 - 2d merges: \"C\", \"L\", \"U\", \"X\" These characteristics can be easily learned by sequence generator networks, as we demonstrate further below. We ﬁnd strong indications that this pattern Optimized Table Tokenization for Table Structure Recognition 7 1 - simple cells: \"C\" 2 - horizontal merges: \"C\", \"L\" 3 - vertical merges: \"C\", \"U\" 4 - 2d merges: \"C\", \"L\", \"U\", \"X\" We ﬁnd strong indications that this pattern Optimized Table Tokenization for Table Structure Recognition 7 1 - simple cells: \"C\" 2 - horizontal merges: \"C\", \"L\" 3 - vertical merges: \"C\", \"U\" 4 - 2d merges: \"C\", \"L\", \"U\", \"X\"",
    "token_count": 268,
    "figures": "[\"\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\", \"C\\tL\\tC\\tL\\tL\\nU\\tX\\tC\\tC\\tC\\nC\\tC\\tC\\tC\\tC\\nU\\tC\\tC\\tC\\tC\\nU\\tC\\tC\\tC\\tC\", \"\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\\n\\t\\t\\t\\t\", \"C\\nU\\n\\nU\", \"C\\tL\\t\\tL\\tL\\nU\\tX X X\\nX X X\\t\\t\\t\\n\\t\\t\\t\\t\\nU\\t\\t\\t\\t\", \"C\\t\\tL\\tL\\tL\", \"2 [q0 4 Total benig melignant observer 2 3 13 PI 15 malignant 62 62 Total ohserwer 1 13 64\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 7,
    "timestamp": "2025-09-27T16:43:49.157731Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p8_0",
    "chunk_type": "pdf_page_chunk",
    "text": "1. Item 2. Item 3. Item 4. Item Amount Names 1000 500 3500 150 unit unit unit unit 8 M. Lysak, et al.5 ExperimentsExtracted Table Images Standardized Images 4.3 Error-detection and -mitigation Encoder BBox Decoder Structure Decoder reduces signiﬁcantly the column drift seen in the HTML based models (see Fig- ure 5). The design of OTSL allows to validate a table structure easily on an unﬁnished sequence. The detection of an invalid sequence token is a clear indication of a prediction mistake, however a valid sequence by itself does not guarantee pre- diction correctness. Diﬀerent heuristics can be used to correct token errors in an invalid sequence and thus increase the chances for accurate predictions. Such heuristics can be applied either after the prediction of each token, or at the end on the entire predicted sequence. For example a simple heuristic which can cor- rect the predicted OTSL sequence on-the-ﬂy is to verify if the token with the highest prediction conﬁdence invalidates the predicted sequence, and replace it by the token with the next highest conﬁdence until OTSL rules are satisﬁed. To evaluate the impact of OTSL on prediction accuracy and inference times, we conducted a series of experiments based on the TableFormer model (Figure 4) with two objectives: Firstly we evaluate the prediction quality and performance of OTSL vs. HTML after performing Hyper Parameter Optimization (HPO) on the canonical PubTabNet data set. Secondly we pick the best hyper-parameters found in the ﬁrst step and evaluate how OTSL impacts the performance of TableFormer after training on other publicly available data sets (FinTabNet, PubTables-1M [14]). The ground truth (GT) from all data sets has been con- verted into OTSL format for this purpose, and will be made publicly available. Fig. 4. Architecture sketch of the TableFormer model, which is a representative for the Im2Seq approach. We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Inter- section Over Union (IOU) threshold for the bounding-box predictions of table cells.",
    "token_count": 500,
    "figures": "[\"C\\tC\\tC\\nC\\tC\\tC\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 8,
    "timestamp": "2025-09-27T16:43:49.704739Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p8_1",
    "chunk_type": "pdf_page_chunk",
    "text": "Architecture sketch of the TableFormer model, which is a representative for the Im2Seq approach. We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Inter- section Over Union (IOU) threshold for the bounding-box predictions of table cells. The predicted OTSL structures were converted back to HTML format in BBoxes 2 1 L 3 C C C C C C C C [x 1 , y 2 , x 2 , y 2 ] [x 1 ', y 2 ', x 2 ', y 2 '] [x 1 '', y 2' ', x 2 '', y 2 ''] ... BBoxes in sync with tag sequence NL NL NL NL NL the table structure Structure Tags in OTSL format 1 2 3 3 BBoxes can be traced back to the original image to extract content Structure Tags sequence provide full description of 2 1",
    "token_count": 198,
    "figures": "[\"C\\tC\\tC\\nC\\tC\\tC\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 8,
    "timestamp": "2025-09-27T16:43:49.705354Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p8_2",
    "chunk_type": "pdf_page_chunk",
    "text": "We rely on standard metrics such as Tree Edit Distance score (TEDs) for table structure prediction, and Mean Average Precision (mAP) with 0.75 Inter- section Over Union (IOU) threshold for the bounding-box predictions of table cells. The predicted OTSL structures were converted back to HTML format in BBoxes 2 1 L 3 C C C C C C C C [x 1 , y 2 , x 2 , y 2 ] [x 1 ', y 2 ', x 2 ', y 2 '] [x 1 '', y 2' ', x 2 '', y 2 ''] ... BBoxes in sync with tag sequence NL NL NL NL NL the table structure Structure Tags in OTSL format 1 2 3 3 BBoxes can be traced back to the original image to extract content Structure Tags sequence provide full description of 2 1",
    "token_count": 176,
    "figures": "[\"C\\tC\\tC\\nC\\tC\\tC\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 8,
    "timestamp": "2025-09-27T16:43:49.705769Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p8_3",
    "chunk_type": "pdf_page_chunk",
    "text": "The predicted OTSL structures were converted back to HTML format in BBoxes 2 1 L 3 C C C C C C C C [x 1 , y 2 , x 2 , y 2 ] [x 1 ', y 2 ', x 2 ', y 2 '] [x 1 '', y 2' ', x 2 '', y 2 ''] ... BBoxes in sync with tag sequence NL NL NL NL NL the table structure Structure Tags in OTSL format 1 2 3 3 BBoxes can be traced back to the original image to extract content Structure Tags sequence provide full description of 2 1",
    "token_count": 123,
    "figures": "[\"C\\tC\\tC\\nC\\tC\\tC\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 8,
    "timestamp": "2025-09-27T16:43:49.711732Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p9_0",
    "chunk_type": "pdf_page_chunk",
    "text": "5.1 Hyper Parameter Optimization# enc-layers5.2 Quantitative Results Optimized Table Tokenization for Table Structure Recognition 9 order to compute the TED score. Inference timing results for all experiments were obtained from the same machine on a single core with AMD EPYC 7763 CPU @2.45 GHz. We have chosen the PubTabNet data set to perform HPO, since it includes a highly diverse set of tables. Also we report TED scores separately for simple and complex tables (tables with cell spans). Results are presented in Table. 1. It is evident that with OTSL, our model achieves the same TED score and slightly better mAP scores in comparison to HTML. However OTSL yields a 2x speed up in the inference runtime over HTML. Table 1. HPO performed in OTSL and HTML representation on the same transformer-based TableFormer [9] architecture, trained only on PubTabNet [22]. Ef- fects of reducing the # of layers in encoder and decoder stages of the model show that smaller models trained on OTSL perform better, especially in recognizing complex table structures, and maintain a much higher mAP score than the HTML counterpart. We picked the model parameter conﬁguration that produced the best prediction quality (enc=6, dec=6, heads=8) with PubTabNet alone, then independently trained and evaluated it on three publicly available data sets: PubTabNet (395k samples), FinTabNet (113k samples) and PubTables-1M (about 1M samples). Performance results are presented in Table. 2. It is clearly evident that the model trained on OTSL outperforms HTML across the board, keeping high TEDs and mAP scores even on diﬃcult ﬁnancial tables (FinTabNet) that contain sparse and large tables. Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves signiﬁcantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation. Additionally, the results show that OTSL has an advantage over HTML when applied on a bigger data set like PubTables-1M and achieves signiﬁcantly improved scores. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation. Finally, OTSL achieves faster inference due to fewer decoding steps which is a result of the reduced sequence representation.",
    "token_count": 538,
    "figures": "[\"#\\ndec-layers\\tLanguage\\tTEDs\\t\\t\\tmAP\\n(0.75)\\n\\t\\tsimple\\tcomplex\\tall\\t\\n6\\tOTSL\\nHTML\\t0.965\\n0.969\\t0.934\\n0.927\\t0.955\\n0.955\\t0.88\\n0.857\\n4\\tOTSL\\nHTML\\t0.938\\n0.952\\t0.904\\n0.909\\t0.927\\n0.938\\t0.853\\n0.843\\n4\\tOTSL\\nHTML\\t0.923\\n0.945\\t0.897\\n0.901\\t0.915\\n0.931\\t0.859\\n0.834\\n2\\tOTSL\\nHTML\\t0.952\\n0.944\\t0.92\\n0.903\\t0.942\\n0.931\\t0.857\\n0.824\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 9,
    "timestamp": "2025-09-27T16:43:50.003071Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p10_0",
    "chunk_type": "pdf_page_chunk",
    "text": "D HTML 10 M. Lysak, et al.5.3 Qualitative ResultsA<table> OTSL model shows clean bounding box alignment E OTSL <tr><td></td><td colspan=\"4\"></td><td colspan=\"6\"></td><td colspan=\"3\"></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> <tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> </table> Table 2.",
    "token_count": 495,
    "figures": "[\"Language\\tTEDs\\t\\t\\tmAP(0.75)\\n\\tsimple\\tcomplex\\tall\\t\\nOTSL\\nHTML\\t0.965\\n0.969\\t0.934\\n0.927\\t0.955\\n0.955\\t0.88\\n0.857\\nOTSL\\nHTML\\t0.955\\n0.917\\t0.961\\n0.922\\t0.959\\n0.92\\t0.862\\n0.722\\nOTSL\\nHTML\\t0.987\\n0.983\\t0.964\\n0.944\\t0.977\\n0.966\\t0.896\\n0.889\", \"3 18 \\u5e97 1113 1\", \"H 94 2.6 9.2 85\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 10,
    "timestamp": "2025-09-27T16:43:56.164931Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p10_1",
    "chunk_type": "pdf_page_chunk",
    "text": "<tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr> </table> Table 2. TSR and cell detection results compared between OTSL and HTML on the PubTabNet [22], FinTabNet [21] and PubTables-1M [14] data sets using Table- Former [9] (with enc=6, dec=6, heads=8).To illustrate the qualitative diﬀerences between OTSL and HTML, Figure 5 demonstrates less overlap and more accurate bounding boxes with OTSL. In Figure 6, OTSL proves to be more eﬀective in handling tables with longer to- ken sequences, resulting in even more precise structure prediction and bounding boxes. Fig. 5. The OTSL model produces more accurate bounding boxes with less over- lap (E) than the HTML model (D), when predicting the structure of a sparse ta- ble (A), at twice the inference speed because of shorter sequence length (B),(C). \" PMC2807444_006_00.png\" PubTabNet. S I R ST 0.03 0.06 0.12 0.25 0.5 1 2 4 8 16 32 64 ≥ 128 63 1 1 3 199 5 1 2 4 1 1 416 4 230 1 9 1 1 276 2 12 1 320 1 4 20 2013 3 HTML # tokens: 258 OTSL # tokens: 135 B CHTML model shows bounding box drifting C C L L L C L L L L L C L L NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL",
    "token_count": 487,
    "figures": "[\"Language\\tTEDs\\t\\t\\tmAP(0.75)\\n\\tsimple\\tcomplex\\tall\\t\\nOTSL\\nHTML\\t0.965\\n0.969\\t0.934\\n0.927\\t0.955\\n0.955\\t0.88\\n0.857\\nOTSL\\nHTML\\t0.955\\n0.917\\t0.961\\n0.922\\t0.959\\n0.92\\t0.862\\n0.722\\nOTSL\\nHTML\\t0.987\\n0.983\\t0.964\\n0.944\\t0.977\\n0.966\\t0.896\\n0.889\", \"3 18 \\u5e97 1113 1\", \"H 94 2.6 9.2 85\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 10,
    "timestamp": "2025-09-27T16:43:56.165669Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p10_2",
    "chunk_type": "pdf_page_chunk",
    "text": "PMC2807444_006_00.png\" PubTabNet. S I R ST 0.03 0.06 0.12 0.25 0.5 1 2 4 8 16 32 64 ≥ 128 63 1 1 3 199 5 1 2 4 1 1 416 4 230 1 9 1 1 276 2 12 1 320 1 4 20 2013 3 HTML # tokens: 258 OTSL # tokens: 135 B CHTML model shows bounding box drifting C C L L L C L L L L L C L L NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL",
    "token_count": 229,
    "figures": "[\"Language\\tTEDs\\t\\t\\tmAP(0.75)\\n\\tsimple\\tcomplex\\tall\\t\\nOTSL\\nHTML\\t0.965\\n0.969\\t0.934\\n0.927\\t0.955\\n0.955\\t0.88\\n0.857\\nOTSL\\nHTML\\t0.955\\n0.917\\t0.961\\n0.922\\t0.959\\n0.92\\t0.862\\n0.722\\nOTSL\\nHTML\\t0.987\\n0.983\\t0.964\\n0.944\\t0.977\\n0.966\\t0.896\\n0.889\", \"3 18 \\u5e97 1113 1\", \"H 94 2.6 9.2 85\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 10,
    "timestamp": "2025-09-27T16:43:56.166228Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p10_3",
    "chunk_type": "pdf_page_chunk",
    "text": "S I R ST 0.03 0.06 0.12 0.25 0.5 1 2 4 8 16 32 64 ≥ 128 63 1 1 3 199 5 1 2 4 1 1 416 4 230 1 9 1 1 276 2 12 1 320 1 4 20 2013 3 HTML # tokens: 258 OTSL # tokens: 135 B CHTML model shows bounding box drifting C C L L L C L L L L L C L L NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL C C C C C C C C C C C C C C NL",
    "token_count": 213,
    "figures": "[\"Language\\tTEDs\\t\\t\\tmAP(0.75)\\n\\tsimple\\tcomplex\\tall\\t\\nOTSL\\nHTML\\t0.965\\n0.969\\t0.934\\n0.927\\t0.955\\n0.955\\t0.88\\n0.857\\nOTSL\\nHTML\\t0.955\\n0.917\\t0.961\\n0.922\\t0.959\\n0.92\\t0.862\\n0.722\\nOTSL\\nHTML\\t0.987\\n0.983\\t0.964\\n0.944\\t0.977\\n0.966\\t0.896\\n0.889\", \"3 18 \\u5e97 1113 1\", \"H 94 2.6 9.2 85\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 10,
    "timestamp": "2025-09-27T16:43:56.166493Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p11_0",
    "chunk_type": "pdf_page_chunk",
    "text": "BOTSL HTML C Repeating pattern of horizontally merged cells A Optimized Table Tokenization for Table Structure Recognition 11 Fig. 6. Visualization of predicted structure and detected bounding boxes on a complex table with many rows. The OTSL model (B) captured repeating pattern of horizontally merged cells from the GT (A), unlike the HTML model (C). The HTML model also didn’t complete the HTML sequence correctly and displayed a lot more of drift and overlap of bounding boxes. \" PMC5406406_003_01.png\" PubTabNet. Repeating pattern is well represented in predictionsBounding box drifting at the end Horizontally merged cells are not present Incorrect end of HTML sequence PMC5406406_003_01.png\" PubTabNet. Repeating pattern is well represented in predictionsBounding box drifting at the end Horizontally merged cells are not present Incorrect end of HTML sequence Repeating pattern is well represented in predictionsBounding box drifting at the end Horizontally merged cells are not present Incorrect end of HTML sequence",
    "token_count": 221,
    "figures": "[\"Dental caries Overall Yes No Characteristic f % % f % p-Value Demographics oflenfue /aung 0.42 Spanish 72 80 38 83 27 73 Engish 18 20 8 17 10 27 Immigrant parents 0.38 Yos 44 47 22 48 22 59 No 46 53 24 52 15 41 Years perents have Iived in the United States 0.78 010 22 24 11 24 8 22 1120 35 39 19 41 13 35 >20 33 37 16 35 16 43 Householdl income 0.66 <$15,000 48 53 26 56 19 51 $15,000$30,000 42 47 20 44 18 49 Parents\\u2019 highest level of education 1.0 8th grade 43 48 22 48 17 46 12th grade 27 30 13 28 11 30 Some college 20 22 11 24 9 24 Dental hygiene and history Chilidren recelve fluoride varnish 0.01' Yos 38 49 13 31 18 64 No 39 51 29 69 10 36 Household uses fluorinated mouthwash <0.001\\u00b0 Yes 39 43 28 61 8 22 No 51 57 18 39 29 78 Parents recelved fluoride education 0.01' Yes 32 36 18 40 26 72 No 58 64 27 09 10 28 Parents report hasving dental caries 0.21 Yes 77 86 37 80 34 92 No 13 14 9 20 3 8 Parents visit the dlentist <0.001' Ae 59 66 19 41 33 89 Never 31 34 27 59 4 11\", \"eaefes 1 160 170 184 1542 2:00 20:8 215 16 C 2-1 DG 87 BO 34 D2 No -4 20 Parants vilsit the derd ist 0.001 Yeary DG 19 41 33 Nevor 3-4 2T 50 1\", \"0 1 2 3 4 5 9 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203\", \"2 Doatalcaries Overal 10 11 12 1a 14 1519 127 it fhe derd lst 0\", \"0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221\"]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 11,
    "timestamp": "2025-09-27T16:44:28.291537Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": true,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p12_0",
    "chunk_type": "pdf_page_chunk",
    "text": "12 M. Lysak, et al. 6 ConclusionReferences We demonstrated that representing tables in HTML for the task of table struc- ture recognition with Im2Seq models is ill-suited and has serious limitations. Furthermore, we presented in this paper an Optimized Table Structure Language (OTSL) which, when compared to commonly used general purpose languages, has several key beneﬁts. First and foremost, given the same network conﬁguration, inference time for a table-structure prediction is about 2 times faster compared to the conventional HTML approach. This is primarily owed to the shorter sequence length of the OTSL representation. Additional performance beneﬁts can be obtained with HPO (hyper parameter optimization). As we demonstrate in our experiments, models trained on OTSL can be signiﬁcantly smaller, e.g. by reducing the number of encoder and decoder layers, while preserving comparatively good prediction quality. This can further improve inference performance, yielding 5-6 times faster inference speed in OTSL with prediction quality comparable to models trained on HTML (see Table 1). Secondly, OTSL has more inherent structure and a signiﬁcantly restricted vo- cabulary size. This allows autoregressive models to perform better in the TED metric, but especially with regards to prediction accuracy of the table-cell bound- ing boxes (see Table 2). As shown in Figure 5, we observe that the OTSL dras- tically reduces the drift for table cell bounding boxes at high row count and in sparse tables. This leads to more accurate predictions and a signiﬁcant reduction in post-processing complexity, which is an undesired necessity in HTML-based Im2Seq models. Signiﬁcant novelty lies in OTSL syntactical rules, which are few, simple and always backwards looking. Each new token can be validated only by analyzing the sequence of previous tokens, without requiring the entire sequence to detect mistakes. This in return allows to perform structural error detection and correction on-the-ﬂy during sequence generation.1. Auer, C., Dolﬁ, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering doc- ument conversion as a cloud service with high throughput and responsiveness. CoRR abs/2206.00785 (2022).",
    "token_count": 509,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 12,
    "timestamp": "2025-09-27T16:44:28.569224Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p12_1",
    "chunk_type": "pdf_page_chunk",
    "text": "Auer, C., Dolﬁ, M., Carvalho, A., Ramis, C.B., Staar, P.W.J.: Delivering doc- ument conversion as a cloud service with high throughput and responsiveness. CoRR abs/2206.00785 (2022). https://doi.org/10.48550/arXiv.2206.00785 , https://doi.org/10.48550/arXiv.2206.00785 2. Chen, B., Peng, D., Zhang, J., Ren, Y., Jin, L.: Complex table structure recognition in the wild using transformer and identity matrix-based augmentation. In: Porwal, U., Fornés, A., Shafait, F. (eds.) Frontiers in Handwriting Recognition. pp. 545– 561. Springer International Publishing, Cham (2022) 3. Chi, Z., Huang, H., Xu, H.D., Yu, H., Yin, W., Mao, X.L.: Complicated table structure recognition. arXiv preprint arXiv:1908.04729 (2019) 4. Deng, Y., Rosenberg, D., Mann, G.: Challenges in end-to-end neural scientiﬁc table recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 894–901. IEEE (2019) 894–901. IEEE (2019) IEEE (2019)",
    "token_count": 321,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 12,
    "timestamp": "2025-09-27T16:44:28.569855Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p13_0",
    "chunk_type": "pdf_page_chunk",
    "text": "Optimized Table Tokenization for Table Structure Recognition 13 5. Kayal, P., Anand, M., Desai, H., Singh, M.: Tables to latex: structure and content extraction from scientiﬁc tables. International Journal on Document Analysis and Recognition (IJDAR) pp. 1–10 (2022) 6. Lee, E., Kwon, J., Yang, H., Park, J., Lee, S., Koo, H.I., Cho, N.I.: Table structure recognition based on grid shape graph. In: 2022 Asia-Paciﬁc Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). pp. 1868– 1873. IEEE (2022) 7. Li, M., Cui, L., Huang, S., Wei, F., Zhou, M., Li, Z.: Tablebank: A benchmark dataset for table detection and recognition (2019) 8. Livathinos, N., Berrospi, C., Lysak, M., Kuropiatnyk, V., Nassar, A., Carvalho, A., Dolﬁ, M., Auer, C., Dinkla, K., Staar, P.: Robust pdf document conversion using recurrent neural networks. Proceedings of the AAAI Conference on Artiﬁcial Intelligence 35 (17), 15137–15145 (May 2021), https://ojs.aaai.org/index.php/ AAAI/article/view/17777 9. Nassar, A., Livathinos, N., Lysak, M., Staar, P.: Tableformer: Table structure un- derstanding with transformers. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4614–4623 (June 2022) 10. Pﬁtzmann, B., Auer, C., Dolﬁ, M., Nassar, A.S., Staar, P.W.J.: Doclaynet: A large human-annotated dataset for document-layout segmentation. In: Zhang, A., Rangwala, H. (eds.) KDD ’22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Washington, DC, USA, August 14 - 18, 2022. pp. 3743–3751.",
    "token_count": 508,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 13,
    "timestamp": "2025-09-27T16:44:28.851352Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p13_1",
    "chunk_type": "pdf_page_chunk",
    "text": "pp. 3743–3751. ACM (2022). https://doi.org/10.1145/3534678.3539043 , https:// doi.org/10.1145/3534678.3539043 11. Prasad, D., Gadpal, A., Kapadni, K., Visave, M., Sultanpure, K.: Cascadetabnet: An approach for end to end table detection and structure recognition from image- based documents. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. pp. 572–573 (2020) 12. Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: Deep learning for detection and structure recognition of tables in document images. In: 2017 14th IAPR international conference on document analysis and recognition (ICDAR). vol. 1, pp. 1162–1167. IEEE (2017) 13. Siddiqui, S.A., Fateh, I.A., Rizvi, S.T.R., Dengel, A., Ahmed, S.: Deeptabstr: Deep learning based table structure recognition. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 1403–1409 (2019). https:// doi.org/10.1109/ICDAR.2019.00226 14. Smock, B., Pesala, R., Abraham, R.: PubTables-1M: Towards comprehensive ta- ble extraction from unstructured documents. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). pp. 4634–4642 (June 2022) 15. Staar, P.W.J., Dolﬁ, M., Auer, C., Bekas, C.: Corpus conversion service: A ma- chine learning platform to ingest documents at scale. In: Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Min- ing. pp. 774–782. KDD ’18, Association for Computing Machinery, New York, NY, USA (2018). https://doi.org/10.1145/3219819.3219834 , https://doi.org/10. 1145/3219819.3219834 16.",
    "token_count": 507,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 13,
    "timestamp": "2025-09-27T16:44:28.851904Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p13_2",
    "chunk_type": "pdf_page_chunk",
    "text": "https://doi.org/10.1145/3219819.3219834 , https://doi.org/10. 1145/3219819.3219834 16. Wang, X.: Tabular Abstraction, Editing, and Formatting. Ph.D. thesis, CAN (1996), aAINN09397 17. Xue, W., Li, Q., Tao, D.: Res2tim: Reconstruct syntactic structures from table images. In: 2019 International Conference on Document Analysis and Recognition (ICDAR). pp. 749–755. IEEE (2019) 749–755. IEEE (2019) IEEE (2019)",
    "token_count": 142,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 13,
    "timestamp": "2025-09-27T16:44:28.852217Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
,
  {
    "document_id": "fcf88f599fae76d59a778aa97c903a27",
    "file_name": "2305.03393v1.pdf",
    "chunk_id": "fcf88f599fae76d59a778aa97c903a27_p14_0",
    "chunk_type": "pdf_page_chunk",
    "text": "14 M. Lysak, et al. 18. Xue, W., Yu, B., Wang, W., Tao, D., Li, Q.: Tgrnet: A table graph reconstruc- tion network for table structure recognition. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 1295–1304 (2021) 19. Ye, J., Qi, X., He, Y., Chen, Y., Gu, D., Gao, P., Xiao, R.: Pingan-vcgroup’s solution for icdar 2021 competition on scientiﬁc literature parsing task b: Ta- ble recognition to html (2021). https://doi.org/10.48550/ARXIV.2105.01848 , https://arxiv.org/abs/2105.01848 20. Zhang, Z., Zhang, J., Du, J., Wang, F.: Split, embed and merge: An accurate table structure recognizer. Pattern Recognition 126 , 108565 (2022) 21. Zheng, X., Burdick, D., Popa, L., Zhong, X., Wang, N.X.R.: Global table extractor (gte): A framework for joint table identiﬁcation and cell structure recognition using visual context. In: 2021 IEEE Winter Conference on Applications of Computer Vi- sion (WACV). pp. 697–706 (2021). https://doi.org/10.1109/WACV48630.2021. 00074 22. Zhong, X., ShaﬁeiBavani, E., Jimeno Yepes, A.: Image-based table recognition: Data, model, and evaluation. In: Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M. (eds.) Computer Vision – ECCV 2020. pp. 564–580. Springer International Pub- lishing, Cham (2020) 23. Zhong, X., Tang, J., Yepes, A.J.: Publaynet: largest dataset ever for document lay- out analysis. In: 2019 International Conference on Document Analysis and Recog- nition (ICDAR). pp. 1015–1022. IEEE (2019) 1015–1022. IEEE (2019) IEEE (2019)",
    "token_count": 507,
    "figures": "[]",
    "file_type": "application/pdf",
    "source_url": "s3://e2e-rag-system-42/data/raw/pdfs/2305.03393v1.pdf",
    "page_number": 14,
    "timestamp": "2025-09-27T16:44:29.009490Z",
    "parser_version": "pdf-v1",
    "tags": [],
    "layout_tags": "[]",
    "used_ocr": false,
    "heading_path": "[]",
    "headings": "[]"
  }
]
