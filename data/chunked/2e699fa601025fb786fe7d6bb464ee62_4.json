{
  "document_id": "2e699fa601025fb786fe7d6bb464ee62",
  "chunk_id": "2e699fa601025fb786fe7d6bb464ee62_4",
  "chunk_type": "token_window",
  "text": "**nemoretriever-parse**: Model used specifically for section-level chunking, as it can intelligently detect section headers and document structure. For a fair comparison between page-level and section-level chunking, we also used nemoretriever-parse extraction for a version of page-level chunking tests. This dual-framework approach enabled us to leverage the strengths of each tool while ensuring that our chunking strategy comparisons reflected true performance differences rather than extraction artifacts. When comparing page-level versus section-level chunking, we used the same nemoretriever-parse extraction to eliminate any extraction-based variances, providing a more controlled comparison. It’s important to note that while our chunking strategies (page-level, section-level, and token-based) were applied to the text content of documents, tables, and charts were extracted as separate entities. These elements were not split or chunked, but rather preserved as complete units to maintain their integrity and context. This approach ensures that complex information remains intact during the retrieval process, allowing the RAG system to access complete tables and charts when needed. ### RAG system implementation For our experiments, we used components from the NVIDIA RAG Blueprint, which provides a comprehensive reference implementation for enterprise-grade RAG pipelines. This blueprint offers: - A modular microservices architecture that enables easy component swapping and evaluation. - Support for multimodal data processing (documents with text, images, charts, and tables). - Integration with advanced NeMo Retriever microservices for embedding, reranking, and LLM inference - Extensive configurability for experimentation with different hyperparameters. The NVIDIA RAG Blueprint is particularly well-suited for chunking experiments as it provides: - Multiple pre-built chunking strategies. - Vector database integration for efficient storage and retrieval. - Robust evaluation capabilities to measure performance differences. If you’re conducting chunking experiments or building a production RAG system, this blueprint can serve as an excellent starting point, reducing development time while providing the flexibility needed for customization. To ensure a fair and robust evaluation across chunking strategies, we standardized the following components in our RAG pipeline: **Embedding model**: nvidia/llama-3.2-nv-embedqa-1b-v2**Reranking model**: nvidia/llama-3.2-nv-rerankqa-1b-v2**Retrieval top-k**: 10 (number of retrieved contexts for generation)**Generator model**: nvidia/llama-3.",
  "token_count": 500,
  "embedding": null,
  "file_type": "text/html",
  "source_url": "s3://e2e-rag-system-42/data/raw/htmls/best_chunking_stratergy.html",
  "page_number": null,
  "slide_range": null,
  "row_range": null,
  "token_range": [
    1350,
    1850
  ],
  "audio_range": null,
  "timestamp": "2025-09-17T12:50:02Z",
  "parser_version": "trafilatura-only-v2",
  "tags": [],
  "layout_tags": [
    "page"
  ],
  "used_ocr": false,
  "parse_chunk_duration_ms": 429,
  "heading_path": [],
  "headings": [
    "Finding the Best Chunking Strategy for Accurate AI Responses | NVIDIA Technical Blog"
  ],
  "line_range": null,
  "chunk_duration_ms": null,
  "snapshot_url": null
}